{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aTw-MRtdPcxD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bfe0d4e-3637-4261-c6bc-68b162ad10a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r /content/drive/MyDrive/ECPE/ . # takes ~ 1m 45 s"
      ],
      "metadata": {
        "id": "0rUpmZpLU1Hs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xvzf ECPE/audio_feats.tar.gz &> /dev/null\n",
        "!tar -xvzf ECPE/video_feats.tar.gz &> /dev/null\n",
        "!tar -xvzf ECPE/text_feats.tar.gz &> /dev/null\n",
        "# ~ takes 3 mins"
      ],
      "metadata": {
        "id": "m80cE-aeGSsB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "anno_pth = \"/content/drive/MyDrive/SemEval-2024_Task3/text/Subtask_2_train.json\"\n",
        "anno = json.load(open(anno_pth, \"r\"))"
      ],
      "metadata": {
        "id": "LrGnDPbPQAPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pickle\n",
        "import json\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "class MultiModalDataset(Dataset):\n",
        "    def __init__(self, data_pth):\n",
        "        self.conv_utt_id_list, self.conv_couples_list, self.y_emotions_list, \\\n",
        "        self.y_causes_list, self.conv_len_list, self.conv_id_list \\\n",
        "        = self.read_data(data_pth)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y_emotions_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        conv_couples, y_emotions, y_causes = self.conv_couples_list[idx], self.y_emotions_list[idx], self.y_causes_list[idx]\n",
        "        conv_len, conv_id = self.conv_len_list[idx], self.conv_id_list[idx]\n",
        "        conv_utt_ids = self.conv_utt_id_list[idx]\n",
        "\n",
        "        assert conv_len == len(y_emotions)\n",
        "        assert conv_len == len(conv_utt_ids)\n",
        "\n",
        "        v, a, t, v_lens, a_lens, t_lens = self.load_tensors(conv_id, conv_utt_ids)\n",
        "\n",
        "        return conv_couples, y_emotions, y_causes, conv_len, conv_id, \\\n",
        "               v, a, t, v_lens, a_lens, t_lens\n",
        "\n",
        "    def load_tensors(self, conv_id, utt_ids):\n",
        "      videos, v_lens = [], []\n",
        "      audios, a_lens = [], []\n",
        "      texts, t_lens = [], []\n",
        "\n",
        "      for utt in utt_ids:\n",
        "        id = 'dia'+str(conv_id)+'utt'+str(utt)+'.pkl'\n",
        "\n",
        "        v = torch.tensor(pickle.load(open(\"ECPE/video_features/\"+id, \"rb\")), dtype=torch.float)\n",
        "        a = pickle.load(open(\"ECPE/audio_features/\"+id, \"rb\")).detach()\n",
        "        t = pickle.load(open(\"text_features/\"+id, \"rb\")).squeeze().detach()\n",
        "\n",
        "        videos.append(v); v_lens.append(len(v))\n",
        "        audios.append(a); a_lens.append(len(a))\n",
        "        texts.append(t); t_lens.append(len(t))\n",
        "\n",
        "      videos = pad_sequence(videos, batch_first=True)\n",
        "      audios = pad_sequence(audios, batch_first=True)\n",
        "      texts = pad_sequence(texts, batch_first=True)\n",
        "\n",
        "      # v_mask = self.get_mask(videos, v_lens)\n",
        "      # a_mask = self.get_mask(audios, a_lens)\n",
        "      # t_mask = self.get_mask(texts, t_lens)\n",
        "\n",
        "      return videos, audios, texts, v_lens, a_lens, t_lens\n",
        "\n",
        "    def get_mask(self, seq, lens):\n",
        "      n, max_len, dim = seq.size()\n",
        "      mask = np.zeros([n, max_len])\n",
        "      for idx, seq_len in enumerate(lens):\n",
        "        mask[idx][:seq_len] = 1\n",
        "\n",
        "      return torch.BoolTensor(mask)\n",
        "\n",
        "\n",
        "    def read_data(self, data_pth):\n",
        "        data = json.load(open(data_pth, \"r\"))\n",
        "        conv_id_list = []\n",
        "        conv_len_list = []\n",
        "        conv_utt_id_list = []\n",
        "        conv_couples_list = []\n",
        "        y_emotions_list, y_causes_list = [], []\n",
        "\n",
        "        for conv in data:\n",
        "          if len(conv[\"emotion-cause_pairs\"]) != 0:\n",
        "            conv_id_list.append(conv[\"conversation_ID\"])\n",
        "            utterances = conv[\"conversation\"]\n",
        "            conv_len = len(utterances)\n",
        "            conv_len_list.append(conv_len)\n",
        "            conv_utt_id_list.append([u['utterance_ID'] for u in utterances])\n",
        "\n",
        "            couples = conv[\"emotion-cause_pairs\"]\n",
        "\n",
        "            conv_couples = [[int(e.split('_')[0]), int(c)] for e, c in couples]\n",
        "            conv_emotions, conv_causes = zip(*conv_couples)\n",
        "            conv_couples_list.append(conv_couples)\n",
        "\n",
        "            y_emotions, y_causes = [], []\n",
        "            for i in range(conv_len):\n",
        "                emotion_label = int(i + 1 in conv_emotions)\n",
        "                cause_label = int(i + 1 in conv_causes)\n",
        "                y_emotions.append(emotion_label)\n",
        "                y_causes.append(cause_label)\n",
        "\n",
        "            y_emotions_list.append(y_emotions)\n",
        "            y_causes_list.append(y_causes)\n",
        "\n",
        "        return conv_utt_id_list, conv_couples_list, y_emotions_list, y_causes_list, conv_len_list, conv_id_list,"
      ],
      "metadata": {
        "id": "YjSvbB1UTZVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.sparse as sp\n",
        "\n",
        "TORCH_SEED = 42\n",
        "gen = torch.Generator().manual_seed(TORCH_SEED)\n",
        "\n",
        "def build_loaders(configs, anno_pth, shuffle=True, val_ratio=0.2):\n",
        "    dataset = MultiModalDataset(anno_pth)\n",
        "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [1-val_ratio, val_ratio], gen)\n",
        "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=configs.batch_size,\n",
        "                                               shuffle=shuffle, collate_fn=batch_preprocessing)\n",
        "\n",
        "    val_loader = torch.utils.data.DataLoader(dataset=val_dataset, batch_size=configs.batch_size,\n",
        "                                               shuffle=False, collate_fn=batch_preprocessing)\n",
        "\n",
        "    return train_loader, val_loader\n",
        "\n",
        "def batch_preprocessing(batch):\n",
        "    conv_couples_b, y_emotions_b, y_causes_b, conv_len_b, conv_id_b, \\\n",
        "    v_token_b, a_token_b, t_token_b, v_lens_b, a_lens_b, t_lens_b = zip(*batch)\n",
        "\n",
        "    y_mask_b, y_emotions_b, y_causes_b = pad_convs(conv_len_b, y_emotions_b, y_causes_b)\n",
        "    adj_b = pad_matrices(conv_len_b)\n",
        "    v_token_b = pad_conversations(list(v_token_b), conv_len_b)\n",
        "    a_token_b = pad_conversations(list(a_token_b), conv_len_b)\n",
        "    t_token_b = pad_conversations(list(t_token_b), conv_len_b)\n",
        "\n",
        "    v_mask = get_mask(v_token_b, v_lens_b)\n",
        "    a_mask = get_mask(a_token_b, a_lens_b)\n",
        "    t_mask = get_mask(t_token_b, t_lens_b)\n",
        "\n",
        "    return np.array(conv_len_b), np.array(adj_b), \\\n",
        "           np.array(y_emotions_b), np.array(y_causes_b), np.array(y_mask_b), conv_couples_b, conv_id_b, \\\n",
        "           v_token_b, a_token_b, t_token_b, v_mask, a_mask, t_mask\n",
        "\n",
        "def pad_conversations(seq_tokens, conv_lens):\n",
        "  num_conv = len(seq_tokens)\n",
        "  num_utt = max(conv_lens)\n",
        "  num_tokens = max([s.size()[1] for s in seq_tokens])\n",
        "\n",
        "  for i, seq in enumerate(seq_tokens):\n",
        "    cur_utt, cur_len, _ = seq.size()\n",
        "    pad = (0, 0, 0, num_tokens - cur_len, 0, num_utt - cur_utt)\n",
        "    seq_tokens[i] = F.pad(seq, pad, \"constant\", 0)\n",
        "\n",
        "  return pad_sequence(seq_tokens, batch_first=True)\n",
        "\n",
        "def get_mask(seq, lens):\n",
        "    num_conv, num_utt, max_len, dim = seq.size()\n",
        "    mask = np.zeros([num_conv, num_utt, max_len])\n",
        "\n",
        "    for conv, conv_len in enumerate(lens):\n",
        "      for utt, seq_len in enumerate(conv_len):\n",
        "        mask[conv][utt][:seq_len] = 1\n",
        "\n",
        "    return torch.BoolTensor(mask)\n",
        "\n",
        "def pad_convs(conv_len_b, y_emotions_b, y_causes_b):\n",
        "    max_conv_len = max(conv_len_b)\n",
        "\n",
        "    y_mask_b, y_emotions_b_, y_causes_b_ = [], [], []\n",
        "    for y_emotions, y_causes in zip(y_emotions_b, y_causes_b):\n",
        "        y_emotions_ = pad_list(y_emotions, max_conv_len, -1)\n",
        "        y_causes_ = pad_list(y_causes, max_conv_len, -1)\n",
        "        y_mask = list(map(lambda x: 0 if x == -1 else 1, y_emotions_))\n",
        "\n",
        "        y_mask_b.append(y_mask)\n",
        "        y_emotions_b_.append(y_emotions_)\n",
        "        y_causes_b_.append(y_causes_)\n",
        "\n",
        "    return y_mask_b, y_emotions_b_, y_causes_b_\n",
        "\n",
        "\n",
        "def pad_matrices(conv_len_b):\n",
        "    N = max(conv_len_b)\n",
        "    adj_b = []\n",
        "    for conv_len in conv_len_b:\n",
        "        adj = np.ones((conv_len, conv_len))\n",
        "        adj = sp.coo_matrix(adj)\n",
        "        adj = sp.coo_matrix((adj.data, (adj.row, adj.col)),\n",
        "                            shape=(N, N), dtype=np.float32)\n",
        "        adj_b.append(adj.toarray())\n",
        "    return adj_b\n",
        "\n",
        "\n",
        "def pad_list(element_list, max_len, pad_mark):\n",
        "    element_list_pad = element_list[:]\n",
        "    pad_mark_list = [pad_mark] * (max_len - len(element_list))\n",
        "    element_list_pad.extend(pad_mark_list)\n",
        "    return element_list_pad"
      ],
      "metadata": {
        "id": "eae8Pk77ujcN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install einops transformers"
      ],
      "metadata": {
        "id": "Y8Z0kh_AXL0F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "from torch import nn, einsum\n",
        "from einops import rearrange, repeat, pack\n",
        "from enum import Enum\n",
        "\n",
        "class TokenTypes(Enum):\n",
        "    VIDEO = 0\n",
        "    AUDIO = 1\n",
        "    TEXT = 2\n",
        "    FUSION = 3\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.gamma = nn.Parameter(torch.ones(dim))\n",
        "        self.register_buffer(\"beta\", torch.zeros(dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.layer_norm(x, x.shape[-1:], self.gamma, self.beta)\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        dim_head = 64,\n",
        "        heads = 1\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.scale = dim_head ** -0.5\n",
        "        self.heads = heads\n",
        "        inner_dim = dim_head * heads\n",
        "\n",
        "        self.norm = LayerNorm(dim)\n",
        "\n",
        "        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n",
        "        self.to_kv = nn.Linear(dim, inner_dim * 2, bias = False)\n",
        "        self.to_out = nn.Linear(inner_dim, dim, bias = False)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x,\n",
        "        pad_mask = None,\n",
        "        attn_mask = None\n",
        "    ):\n",
        "        x = self.norm(x)\n",
        "        kv_x = x\n",
        "\n",
        "        q, k, v = (self.to_q(x), *self.to_kv(kv_x).chunk(2, dim = -1))\n",
        "\n",
        "        q, k, v = map(lambda t:\n",
        "                      rearrange(t, 'b n (h d) -> b h n d', h = self.heads),\n",
        "                      (q, k, v))\n",
        "\n",
        "        q = q * self.scale\n",
        "        sim = einsum('b h i d, b h j d -> b h i j', q, k)\n",
        "        # print(\"sim:\",sim.size())\n",
        "        if pad_mask is not None:\n",
        "          # print(pad_mask.size())\n",
        "          sim = sim.masked_fill(pad_mask, -torch.finfo(sim.dtype).max)\n",
        "\n",
        "        if attn_mask is not None:\n",
        "            # print(attn_mask.size())\n",
        "            sim = sim.masked_fill(~attn_mask, -torch.finfo(sim.dtype).max)\n",
        "\n",
        "        attn = sim.softmax(dim = -1)\n",
        "        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
        "\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
        "        return self.to_out(out)\n",
        "\n",
        "class GEGLU(nn.Module):\n",
        "    def forward(self, x):\n",
        "        x, gate = x.chunk(2, dim = -1)\n",
        "        return F.gelu(gate) * x\n",
        "\n",
        "def FeedForward(dim, mult = 4):\n",
        "    inner_dim = int(dim * mult * 2 / 3)\n",
        "    return nn.Sequential(\n",
        "        LayerNorm(dim),\n",
        "        nn.Linear(dim, inner_dim * 2, bias = False),\n",
        "        GEGLU(),\n",
        "        nn.Linear(inner_dim, dim, bias = False)\n",
        "    )\n",
        "\n",
        "class Zorro_AVT(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        depth,\n",
        "        dim_head = 64,\n",
        "        heads = 8,\n",
        "        ff_mult = 4,\n",
        "        num_fusion_tokens = 8,\n",
        "        out_dim = 768\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # fusion tokens\n",
        "        self.num_fusion_tokens = num_fusion_tokens\n",
        "        self.fusion_tokens = nn.Parameter(torch.randn(num_fusion_tokens, dim).cuda())\n",
        "        self.fusion_mask = torch.ones(num_fusion_tokens) == 1\n",
        "        if torch.cuda.is_available():\n",
        "          self.fusion_mask = self.fusion_mask.cuda()\n",
        "        # transformer\n",
        "        self.layers = nn.ModuleList([])\n",
        "\n",
        "        for _ in range(depth):\n",
        "            self.layers.append(nn.ModuleList([\n",
        "                Attention(dim = dim, dim_head = dim_head, heads = heads),\n",
        "                FeedForward(dim = dim, mult = ff_mult)\n",
        "            ]))\n",
        "\n",
        "        self.norm = LayerNorm(dim)\n",
        "        self.out_layer = nn.Linear(dim, out_dim, bias = False)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        video_tokens,\n",
        "        audio_tokens,\n",
        "        text_tokens,\n",
        "        video_mask,\n",
        "        audio_mask,\n",
        "        text_mask\n",
        "    ):\n",
        "        batch, device = video_tokens.shape[0], video_tokens.device\n",
        "\n",
        "        fusion_tokens = repeat(self.fusion_tokens, 'n d -> b n d', b = batch)\n",
        "        fusion_mask = repeat(self.fusion_mask, 'n -> b n', b = batch)\n",
        "\n",
        "        # construct all tokens\n",
        "        video_tokens, audio_tokens, text_tokens, fusion_tokens = \\\n",
        "        map(lambda t:\n",
        "            rearrange(t, 'b ... d -> b (...) d'),\n",
        "            (video_tokens, audio_tokens, text_tokens, fusion_tokens))\n",
        "\n",
        "\n",
        "        tokens, ps = pack((\n",
        "            video_tokens,\n",
        "            audio_tokens,\n",
        "            text_tokens,\n",
        "            fusion_tokens\n",
        "        ), 'b * d')\n",
        "\n",
        "        # construct mask (thus zorro)\n",
        "        token_types = torch.tensor(list((\n",
        "            *((TokenTypes.VIDEO.value,) * video_tokens.shape[-2]),\n",
        "            *((TokenTypes.AUDIO.value,) * audio_tokens.shape[-2]),\n",
        "            *((TokenTypes.TEXT.value,) * text_tokens.shape[-2]),\n",
        "            *((TokenTypes.FUSION.value,) * fusion_tokens.shape[-2]),\n",
        "        )), device = device, dtype = torch.long)\n",
        "\n",
        "        token_types_attend_from = rearrange(token_types, 'i -> i 1')\n",
        "        token_types_attend_to = rearrange(token_types, 'j -> 1 j')\n",
        "\n",
        "        # the logic goes\n",
        "        # every modality, including fusion can attend to self\n",
        "        zorro_mask = token_types_attend_from == token_types_attend_to\n",
        "\n",
        "        # fusion can attend to everything\n",
        "        zorro_mask = zorro_mask | (token_types_attend_from == TokenTypes.FUSION.value)\n",
        "        # print(\"zorro_mask:\", zorro_mask.size())\n",
        "        # construct padding mask\n",
        "        pad_mask = torch.cat((video_mask, audio_mask, text_mask, fusion_mask), -1)[:, None, None, :]\n",
        "        # print(\"pad_mask:\", pad_mask.size())\n",
        "        # attend and feedforward\n",
        "        for attn, ff in self.layers:\n",
        "            tokens = attn(tokens, pad_mask = pad_mask, attn_mask = zorro_mask) + tokens\n",
        "            tokens = ff(tokens) + tokens\n",
        "\n",
        "        tokens = self.norm(tokens)\n",
        "        fusion_tokens = tokens[:, -self.num_fusion_tokens:, :]\n",
        "        pooled_tokens = torch.mean(fusion_tokens, 1)\n",
        "\n",
        "        return self.out_layer(pooled_tokens)"
      ],
      "metadata": {
        "id": "6D5LI2RyA2mo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torch.nn.functional as F\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class GraphAttentionLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    reference: https://github.com/xptree/DeepInf\n",
        "    \"\"\"\n",
        "    def __init__(self, att_head, in_dim, out_dim, dp_gnn, leaky_alpha=0.2):\n",
        "        super(GraphAttentionLayer, self).__init__()\n",
        "        self.in_dim = in_dim\n",
        "        self.out_dim = out_dim\n",
        "        self.dp_gnn = dp_gnn\n",
        "\n",
        "        self.att_head = att_head\n",
        "        self.W = nn.Parameter(torch.Tensor(self.att_head, self.in_dim, self.out_dim))\n",
        "        self.b = nn.Parameter(torch.Tensor(self.out_dim))\n",
        "\n",
        "        self.w_src = nn.Parameter(torch.Tensor(self.att_head, self.out_dim, 1))\n",
        "        self.w_dst = nn.Parameter(torch.Tensor(self.att_head, self.out_dim, 1))\n",
        "        self.leaky_alpha = leaky_alpha\n",
        "        self.init_gnn_param()\n",
        "\n",
        "        assert self.in_dim == self.out_dim*self.att_head\n",
        "        self.H = nn.Linear(self.in_dim, self.in_dim)\n",
        "        init.xavier_normal_(self.H.weight)\n",
        "\n",
        "    def init_gnn_param(self):\n",
        "        init.xavier_uniform_(self.W.data)\n",
        "        init.zeros_(self.b.data)\n",
        "        init.xavier_uniform_(self.w_src.data)\n",
        "        init.xavier_uniform_(self.w_dst.data)\n",
        "\n",
        "    def forward(self, feat_in, adj=None):\n",
        "        batch, N, in_dim = feat_in.size()\n",
        "        assert in_dim == self.in_dim\n",
        "\n",
        "        feat_in_ = feat_in.unsqueeze(1)\n",
        "        h = torch.matmul(feat_in_, self.W)\n",
        "\n",
        "        attn_src = torch.matmul(F.tanh(h), self.w_src)\n",
        "        attn_dst = torch.matmul(F.tanh(h), self.w_dst)\n",
        "        attn = attn_src.expand(-1, -1, -1, N) + attn_dst.expand(-1, -1, -1, N).permute(0, 1, 3, 2)\n",
        "        attn = F.leaky_relu(attn, self.leaky_alpha, inplace=True)\n",
        "\n",
        "        adj = torch.FloatTensor(adj).to(DEVICE)\n",
        "        mask = 1 - adj.unsqueeze(1)\n",
        "        attn.data.masked_fill_(mask.bool(), -999)\n",
        "\n",
        "        attn = F.softmax(attn, dim=-1)\n",
        "        feat_out = torch.matmul(attn, h) + self.b\n",
        "\n",
        "        feat_out = feat_out.transpose(1, 2).contiguous().view(batch, N, -1)\n",
        "        feat_out = F.elu(feat_out)\n",
        "\n",
        "        gate = F.sigmoid(self.H(feat_in))\n",
        "        feat_out = gate * feat_out + (1 - gate) * feat_in\n",
        "\n",
        "        feat_out = F.dropout(feat_out, self.dp_gnn, training=self.training)\n",
        "\n",
        "        return feat_out\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + ' (' + str(self.in_dim) + ' -> ' + str(self.out_dim*self.att_head) + ')'\n",
        "\n",
        "class Network(nn.Module):\n",
        "    def __init__(self, configs):\n",
        "        super(Network, self).__init__()\n",
        "\n",
        "        self.multimodal = Zorro_AVT(1024, 8)\n",
        "        self.gnn = GraphNN(configs)\n",
        "        self.pred = Pre_Predictions(configs)\n",
        "        self.rank = RankNN(configs)\n",
        "        self.pairwise_loss = configs.pairwise_loss\n",
        "\n",
        "    def forward(self, conv_len, adj, v_tokens, a_tokens, t_tokens, v_mask, a_mask, t_mask):\n",
        "\n",
        "        num_utt = v_tokens.size()[1]\n",
        "        conv_utt_h = []\n",
        "        for i in range(num_utt):\n",
        "          v, a, t = v_tokens[:,i,:,:], a_tokens[:,i,:,:], t_tokens[:,i,:,:]\n",
        "          v_m, a_m, t_m = v_mask[:,i,:], a_mask[:,i,:], t_mask[:,i,:]\n",
        "          out = self.multimodal(v, a, t, v_m, a_m, t_m)\n",
        "          conv_utt_h.append(out)\n",
        "        conv_utt_h = torch.stack(conv_utt_h, 1)\n",
        "\n",
        "        conv_utt_h = self.gnn(conv_utt_h, conv_len, adj)\n",
        "        pred_e, pred_c = self.pred(conv_utt_h)\n",
        "\n",
        "        couples_pred, emo_cau_pos = self.rank(conv_utt_h)\n",
        "\n",
        "        return couples_pred, emo_cau_pos, pred_e, pred_c\n",
        "\n",
        "    def loss_rank(self, couples_pred, emo_cau_pos, doc_couples, y_mask, test=False):\n",
        "        couples_true, couples_mask, doc_couples_pred = self.output_util(couples_pred, emo_cau_pos, doc_couples, y_mask, test)\n",
        "\n",
        "        if not self.pairwise_loss:\n",
        "            couples_mask = torch.BoolTensor(couples_mask).to(DEVICE)\n",
        "            couples_true = torch.FloatTensor(couples_true).to(DEVICE)\n",
        "            criterion = nn.BCEWithLogitsLoss(reduction='mean')\n",
        "            couples_true = couples_true.masked_select(couples_mask)\n",
        "            couples_pred = couples_pred.masked_select(couples_mask)\n",
        "            loss_couple = criterion(couples_pred, couples_true)\n",
        "        else:\n",
        "            x1, x2, y = self.pairwise_util(couples_pred, couples_true, couples_mask)\n",
        "            criterion = nn.MarginRankingLoss(margin=1.0, reduction='mean')\n",
        "            loss_couple = criterion(F.tanh(x1), F.tanh(x2), y)\n",
        "\n",
        "        return loss_couple, doc_couples_pred\n",
        "\n",
        "    def output_util(self, couples_pred, emo_cau_pos, doc_couples, y_mask, test=False):\n",
        "        \"\"\"\n",
        "        TODO: combine this function to data_loader\n",
        "        \"\"\"\n",
        "        batch, n_couple = couples_pred.size()\n",
        "\n",
        "        couples_true, couples_mask = [], []\n",
        "        doc_couples_pred = []\n",
        "        for i in range(batch):\n",
        "            y_mask_i = y_mask[i]\n",
        "            max_doc_idx = sum(y_mask_i)\n",
        "\n",
        "            doc_couples_i = doc_couples[i]\n",
        "            couples_true_i = []\n",
        "            couples_mask_i = []\n",
        "            for couple_idx, emo_cau in enumerate(emo_cau_pos):\n",
        "                if emo_cau[0] > max_doc_idx or emo_cau[1] > max_doc_idx:\n",
        "                    couples_mask_i.append(0)\n",
        "                    couples_true_i.append(0)\n",
        "                else:\n",
        "                    couples_mask_i.append(1)\n",
        "                    couples_true_i.append(1 if emo_cau in doc_couples_i else 0)\n",
        "\n",
        "            couples_pred_i = couples_pred[i]\n",
        "            doc_couples_pred_i = []\n",
        "            if test:\n",
        "                if torch.sum(torch.isnan(couples_pred_i)) > 0:\n",
        "                    k_idx = [0] * 3\n",
        "                else:\n",
        "                    _, k_idx = torch.topk(couples_pred_i, k=3, dim=0)\n",
        "                doc_couples_pred_i = [(emo_cau_pos[idx], couples_pred_i[idx].tolist()) for idx in k_idx]\n",
        "\n",
        "            couples_true.append(couples_true_i)\n",
        "            couples_mask.append(couples_mask_i)\n",
        "            doc_couples_pred.append(doc_couples_pred_i)\n",
        "        return couples_true, couples_mask, doc_couples_pred\n",
        "\n",
        "    def loss_pre(self, pred_e, pred_c, y_emotions, y_causes, y_mask):\n",
        "        y_mask = torch.BoolTensor(y_mask).to(DEVICE)\n",
        "        y_emotions = torch.FloatTensor(y_emotions).to(DEVICE)\n",
        "        y_causes = torch.FloatTensor(y_causes).to(DEVICE)\n",
        "\n",
        "        criterion = nn.BCEWithLogitsLoss(reduction='mean')\n",
        "        pred_e = pred_e.masked_select(y_mask)\n",
        "        true_e = y_emotions.masked_select(y_mask)\n",
        "        loss_e = criterion(pred_e, true_e)\n",
        "\n",
        "        pred_c = pred_c.masked_select(y_mask)\n",
        "        true_c = y_causes.masked_select(y_mask)\n",
        "        loss_c = criterion(pred_c, true_c)\n",
        "        return loss_e, loss_c\n",
        "\n",
        "    def pairwise_util(self, couples_pred, couples_true, couples_mask):\n",
        "        \"\"\"\n",
        "        TODO: efficient re-implementation; combine this function to data_loader\n",
        "        \"\"\"\n",
        "        batch, n_couple = couples_pred.size()\n",
        "        x1, x2 = [], []\n",
        "        for i in range(batch):\n",
        "            x1_i_tmp = []\n",
        "            x2_i_tmp = []\n",
        "            couples_mask_i = couples_mask[i]\n",
        "            couples_pred_i = couples_pred[i]\n",
        "            couples_true_i = couples_true[i]\n",
        "            for pred_ij, true_ij, mask_ij in zip(couples_pred_i, couples_true_i, couples_mask_i):\n",
        "                if mask_ij == 1:\n",
        "                    if true_ij == 1:\n",
        "                        x1_i_tmp.append(pred_ij.reshape(-1, 1))\n",
        "                    else:\n",
        "                        x2_i_tmp.append(pred_ij.reshape(-1))\n",
        "            m = len(x2_i_tmp)\n",
        "            n = len(x1_i_tmp)\n",
        "            x1_i = torch.cat([torch.cat(x1_i_tmp, dim=0)] * m, dim=1).reshape(-1)\n",
        "            x1.append(x1_i)\n",
        "            x2_i = []\n",
        "            for _ in range(n):\n",
        "                x2_i.extend(x2_i_tmp)\n",
        "            x2_i = torch.cat(x2_i, dim=0)\n",
        "            x2.append(x2_i)\n",
        "\n",
        "        x1 = torch.cat(x1, dim=0)\n",
        "        x2 = torch.cat(x2, dim=0)\n",
        "        y = torch.FloatTensor([1] * x1.size(0)).to(DEVICE)\n",
        "        return x1, x2, y\n",
        "\n",
        "\n",
        "class GraphNN(nn.Module):\n",
        "    def __init__(self, configs):\n",
        "        super(GraphNN, self).__init__()\n",
        "        in_dim = configs.feat_dim\n",
        "        self.gnn_dims = [in_dim] + [int(dim) for dim in configs.gnn_dims.strip().split(',')]\n",
        "\n",
        "        self.gnn_layers = len(self.gnn_dims) - 1\n",
        "        self.att_heads = [int(att_head) for att_head in configs.att_heads.strip().split(',')]\n",
        "        self.gnn_layer_stack = nn.ModuleList()\n",
        "        for i in range(self.gnn_layers):\n",
        "            in_dim = self.gnn_dims[i] * self.att_heads[i - 1] if i != 0 else self.gnn_dims[i]\n",
        "            self.gnn_layer_stack.append(\n",
        "                GraphAttentionLayer(self.att_heads[i], in_dim, self.gnn_dims[i + 1], configs.dp)\n",
        "            )\n",
        "\n",
        "    def forward(self, doc_sents_h, doc_len, adj):\n",
        "        batch, max_doc_len, _ = doc_sents_h.size()\n",
        "        assert max(doc_len) == max_doc_len\n",
        "\n",
        "        for i, gnn_layer in enumerate(self.gnn_layer_stack):\n",
        "            doc_sents_h = gnn_layer(doc_sents_h, adj)\n",
        "\n",
        "        return doc_sents_h\n",
        "\n",
        "\n",
        "class RankNN(nn.Module):\n",
        "    def __init__(self, configs):\n",
        "        super(RankNN, self).__init__()\n",
        "        self.K = configs.K\n",
        "        self.pos_emb_dim = configs.pos_emb_dim\n",
        "        self.pos_layer = nn.Embedding(2*self.K + 1, self.pos_emb_dim)\n",
        "        nn.init.xavier_uniform_(self.pos_layer.weight)\n",
        "\n",
        "        self.feat_dim = int(configs.gnn_dims.strip().split(',')[-1]) * int(configs.att_heads.strip().split(',')[-1])\n",
        "        self.rank_feat_dim = 2*self.feat_dim + self.pos_emb_dim\n",
        "        self.rank_layer1 = nn.Linear(self.rank_feat_dim, self.rank_feat_dim)\n",
        "        self.rank_layer2 = nn.Linear(self.rank_feat_dim, 1)\n",
        "\n",
        "    def forward(self, doc_sents_h):\n",
        "        batch, _, _ = doc_sents_h.size()\n",
        "        couples, rel_pos, emo_cau_pos = self.couple_generator(doc_sents_h, self.K)\n",
        "\n",
        "        rel_pos = rel_pos + self.K\n",
        "        rel_pos_emb = self.pos_layer(rel_pos)\n",
        "        kernel = self.kernel_generator(rel_pos)\n",
        "        kernel = kernel.unsqueeze(0).expand(batch, -1, -1)\n",
        "        rel_pos_emb = torch.matmul(kernel, rel_pos_emb)\n",
        "        couples = torch.cat([couples, rel_pos_emb], dim=2)\n",
        "\n",
        "        couples = F.relu(self.rank_layer1(couples))\n",
        "        couples_pred = self.rank_layer2(couples)\n",
        "        return couples_pred.squeeze(2), emo_cau_pos\n",
        "\n",
        "    def couple_generator(self, H, k):\n",
        "        batch, seq_len, feat_dim = H.size()\n",
        "        P_left = torch.cat([H] * seq_len, dim=2)\n",
        "        P_left = P_left.reshape(-1, seq_len * seq_len, feat_dim)\n",
        "        P_right = torch.cat([H] * seq_len, dim=1)\n",
        "        P = torch.cat([P_left, P_right], dim=2)\n",
        "\n",
        "        base_idx = np.arange(1, seq_len + 1)\n",
        "        emo_pos = np.concatenate([base_idx.reshape(-1, 1)] * seq_len, axis=1).reshape(1, -1)[0]\n",
        "        cau_pos = np.concatenate([base_idx] * seq_len, axis=0)\n",
        "\n",
        "        rel_pos = cau_pos - emo_pos\n",
        "        rel_pos = torch.LongTensor(rel_pos).to(DEVICE)\n",
        "        emo_pos = torch.LongTensor(emo_pos).to(DEVICE)\n",
        "        cau_pos = torch.LongTensor(cau_pos).to(DEVICE)\n",
        "\n",
        "        if seq_len > k + 1:\n",
        "            rel_mask = np.array(list(map(lambda x: -k <= x <= k, rel_pos.tolist())), dtype=np.int)\n",
        "            rel_mask = torch.BoolTensor(rel_mask).to(DEVICE)\n",
        "            rel_pos = rel_pos.masked_select(rel_mask)\n",
        "            emo_pos = emo_pos.masked_select(rel_mask)\n",
        "            cau_pos = cau_pos.masked_select(rel_mask)\n",
        "\n",
        "            rel_mask = rel_mask.unsqueeze(1).expand(-1, 2 * feat_dim)\n",
        "            rel_mask = rel_mask.unsqueeze(0).expand(batch, -1, -1)\n",
        "            P = P.masked_select(rel_mask)\n",
        "            P = P.reshape(batch, -1, 2 * feat_dim)\n",
        "        assert rel_pos.size(0) == P.size(1)\n",
        "        rel_pos = rel_pos.unsqueeze(0).expand(batch, -1)\n",
        "\n",
        "        emo_cau_pos = []\n",
        "        for emo, cau in zip(emo_pos.tolist(), cau_pos.tolist()):\n",
        "            emo_cau_pos.append([emo, cau])\n",
        "        return P, rel_pos, emo_cau_pos\n",
        "\n",
        "    def kernel_generator(self, rel_pos):\n",
        "        n_couple = rel_pos.size(1)\n",
        "        rel_pos_ = rel_pos[0].type(torch.FloatTensor).to(DEVICE)\n",
        "        kernel_left = torch.cat([rel_pos_.reshape(-1, 1)] * n_couple, dim=1)\n",
        "        kernel = kernel_left - kernel_left.transpose(0, 1)\n",
        "        return torch.exp(-(torch.pow(kernel, 2)))\n",
        "\n",
        "\n",
        "class Pre_Predictions(nn.Module):\n",
        "    def __init__(self, configs):\n",
        "        super(Pre_Predictions, self).__init__()\n",
        "        self.feat_dim = int(configs.gnn_dims.strip().split(',')[-1]) * int(configs.att_heads.strip().split(',')[-1])\n",
        "        self.out_e = nn.Linear(self.feat_dim, 1)\n",
        "        self.out_c = nn.Linear(self.feat_dim, 1)\n",
        "\n",
        "    def forward(self, doc_sents_h):\n",
        "        pred_e = self.out_e(doc_sents_h)\n",
        "        pred_c = self.out_c(doc_sents_h)\n",
        "        return pred_e.squeeze(2), pred_c.squeeze(2)"
      ],
      "metadata": {
        "id": "N_N_6H3EDTcL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle, json, decimal, math\n",
        "\n",
        "\n",
        "def to_np(x):\n",
        "    return x.data.cpu().numpy()\n",
        "\n",
        "\n",
        "def logistic(x):\n",
        "    return 1 / (1 + math.exp(-x))\n",
        "\n",
        "\n",
        "def eval_func(doc_couples_all, doc_couples_pred_all):\n",
        "    tmp_num = {'ec': 0, 'e': 0, 'c': 0}\n",
        "    tmp_den_p = {'ec': 0, 'e': 0, 'c': 0}\n",
        "    tmp_den_r = {'ec': 0, 'e': 0, 'c': 0}\n",
        "\n",
        "    for doc_couples, doc_couples_pred in zip(doc_couples_all, doc_couples_pred_all):\n",
        "        doc_couples = set([','.join(list(map(lambda x: str(x), doc_couple))) for doc_couple in doc_couples])\n",
        "        doc_couples_pred = set([','.join(list(map(lambda x: str(x), doc_couple))) for doc_couple in doc_couples_pred])\n",
        "\n",
        "        tmp_num['ec'] += len(doc_couples & doc_couples_pred)\n",
        "        tmp_den_p['ec'] += len(doc_couples_pred)\n",
        "        tmp_den_r['ec'] += len(doc_couples)\n",
        "\n",
        "        doc_emos = set([doc_couple.split(',')[0] for doc_couple in doc_couples])\n",
        "        doc_emos_pred = set([doc_couple.split(',')[0] for doc_couple in doc_couples_pred])\n",
        "        tmp_num['e'] += len(doc_emos & doc_emos_pred)\n",
        "        tmp_den_p['e'] += len(doc_emos_pred)\n",
        "        tmp_den_r['e'] += len(doc_emos)\n",
        "\n",
        "        doc_caus = set([doc_couple.split(',')[1] for doc_couple in doc_couples])\n",
        "        doc_caus_pred = set([doc_couple.split(',')[1] for doc_couple in doc_couples_pred])\n",
        "        tmp_num['c'] += len(doc_caus & doc_caus_pred)\n",
        "        tmp_den_p['c'] += len(doc_caus_pred)\n",
        "        tmp_den_r['c'] += len(doc_caus)\n",
        "\n",
        "    metrics = {}\n",
        "    for task in ['ec', 'e', 'c']:\n",
        "        p = tmp_num[task] / (tmp_den_p[task] + 1e-8)\n",
        "        r = tmp_num[task] / (tmp_den_r[task] + 1e-8)\n",
        "        f = 2 * p * r / (p + r + 1e-8)\n",
        "        metrics[task] = (p, r, f)\n",
        "\n",
        "    return metrics['ec'], metrics['e'], metrics['c']"
      ],
      "metadata": {
        "id": "uWWtwXoKQvTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def inference_one_batch(batch, model):\n",
        "    conv_len, adj_b, y_emotions_b, y_causes_b, y_mask_b, conv_couples_b, conv_id_b, \\\n",
        "    v_token_b, a_token_b, t_token_b, v_mask, a_mask, t_mask = batch\n",
        "\n",
        "    couples_pred, emo_cau_pos, pred_e, pred_c = model(conv_len, adj_b,\n",
        "                                                      v_token_b, a_token_b, t_token_b,\n",
        "                                                      v_mask, a_mask, t_mask)\n",
        "\n",
        "    loss_e, loss_c = model.loss_pre(pred_e, pred_c, y_emotions_b, y_causes_b, y_mask_b)\n",
        "    loss_couple, conv_couples_pred_b = model.loss_rank(couples_pred, emo_cau_pos, conv_couples_b, y_mask_b, test=True)\n",
        "\n",
        "    return to_np(loss_couple), to_np(loss_e), to_np(loss_c), \\\n",
        "           conv_couples_b, conv_couples_pred_b, conv_id_b\n",
        "\n",
        "def inference_one_epoch(batches, model):\n",
        "    conv_id_all, conv_couples_all, conv_couples_pred_all = [], [], []\n",
        "    for batch in tqdm(batches):\n",
        "        _, _, _, conv_couples, conv_couples_pred, conv_id_b = inference_one_batch(batch, model)\n",
        "        conv_id_all.extend(conv_id_b)\n",
        "        conv_couples_all.extend(conv_couples)\n",
        "        conv_couples_pred_all.extend(conv_couples_pred)\n",
        "\n",
        "    conv_couples_pred_all = lexicon_based_extraction(conv_id_all, conv_couples_pred_all)\n",
        "    metric_ec, metric_e, metric_c = eval_func(conv_couples_all, conv_couples_pred_all)\n",
        "    return metric_ec, metric_e, metric_c, conv_id_all, conv_couples_all, conv_couples_pred_all\n",
        "\n",
        "def lexicon_based_extraction(conv_ids, couples_pred):\n",
        "\n",
        "    couples_pred_filtered = []\n",
        "    for i, (conv_id, couples_pred_i) in enumerate(zip(conv_ids, couples_pred)):\n",
        "        top1, top1_prob = couples_pred_i[0][0], couples_pred_i[0][1]\n",
        "        couples_pred_i_filtered = [top1]\n",
        "\n",
        "        emotional_clauses_i = set([p[0] for p in conversations[conv_id]['pairs']])\n",
        "        for couple in couples_pred_i[1:]:\n",
        "            if couple[0][0] in emotional_clauses_i and logistic(couple[1]) > 0.5:\n",
        "                couples_pred_i_filtered.append(couple[0])\n",
        "\n",
        "        couples_pred_filtered.append(couples_pred_i_filtered)\n",
        "    return couples_pred_filtered"
      ],
      "metadata": {
        "id": "3bYjr7iIPzu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Config(object):\n",
        "    def __init__(self):\n",
        "        self.feat_dim = 768\n",
        "\n",
        "        self.gnn_dims = '192'\n",
        "        self.att_heads = '4'\n",
        "        self.K = 12\n",
        "        self.pos_emb_dim = 50\n",
        "        self.pairwise_loss = False\n",
        "\n",
        "        self.epochs = 15\n",
        "        self.lr = 1e-5\n",
        "        self.batch_size = 1\n",
        "        self.gradient_accumulation_steps = 2\n",
        "        self.dp = 0.1\n",
        "        self.l2 = 1e-5\n",
        "        self.l2_bert = 0.01\n",
        "        self.warmup_proportion = 0.1\n",
        "        self.adam_epsilon = 1e-8"
      ],
      "metadata": {
        "id": "qz48td6vPAk5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "configs = Config()\n",
        "\n",
        "torch.manual_seed(TORCH_SEED)\n",
        "torch.cuda.manual_seed_all(TORCH_SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "train_loader, val_loader = build_loaders(configs, anno_pth)"
      ],
      "metadata": {
        "id": "AMogKTOVwR_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "model = Network(configs).to(DEVICE)\n",
        "\n",
        "params = model.parameters()\n",
        "optimizer = AdamW(params, lr=configs.lr)\n",
        "\n",
        "num_steps_all = len(train_loader) // configs.gradient_accumulation_steps * configs.epochs\n",
        "warmup_steps = int(num_steps_all * configs.warmup_proportion)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=num_steps_all)\n",
        "\n",
        "total_params = sum([p.numel() for p in filter(lambda p: p.requires_grad, model.parameters())])\n",
        "print(f\"Total trainable parameters in model: {total_params:,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KoOEh9T6R_WN",
        "outputId": "6bc581c6-fb2f-40ba-8250-0ce8b514e148"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total trainable parameters in model: 88,385,229\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "model.zero_grad()\n",
        "max_ec, max_e, max_c = (-1, -1, -1), None, None\n",
        "metric_ec, metric_e, metric_c = (-1, -1, -1), None, None\n",
        "early_stop_flag = None\n",
        "for epoch in range(1, 30):\n",
        "    print(\"=====Epoch {}=====\".format(epoch))\n",
        "    with tqdm(train_loader, unit='batch') as tepoch:\n",
        "      minibatch = 0\n",
        "      t_loss, l_couple, l_e, l_c = 0.,0.,0.,0.\n",
        "      for train_step, batch in enumerate(tepoch, 1):\n",
        "          minibatch += 1\n",
        "          model.train()\n",
        "          conv_len, adj_b, y_emotions_b, y_causes_b, y_mask_b, conv_couples_b, conv_id_b, \\\n",
        "          v_token_b, a_token_b, t_token_b, v_mask, a_mask, t_mask = batch\n",
        "\n",
        "          couples_pred, emo_cau_pos, pred_e, pred_c = model(conv_len, adj_b,\n",
        "                                                            v_token_b.cuda(), a_token_b.cuda(), t_token_b.cuda(),\n",
        "                                                            v_mask.cuda(), a_mask.cuda(), t_mask.cuda())\n",
        "          loss_e, loss_c = model.loss_pre(pred_e, pred_c, y_emotions_b, y_causes_b, y_mask_b)\n",
        "          loss_couple, _ = model.loss_rank(couples_pred, emo_cau_pos, conv_couples_b, y_mask_b)\n",
        "          loss = loss_couple + loss_e + loss_c\n",
        "\n",
        "          loss = loss / configs.gradient_accumulation_steps\n",
        "\n",
        "          loss.backward()\n",
        "          t_loss = (t_loss + loss.item())/minibatch\n",
        "          l_couple = (l_couple + loss_couple.item())/minibatch\n",
        "          l_e = (l_e + loss_e.item())/minibatch\n",
        "          l_c = (l_c + loss_c.item())/minibatch\n",
        "          tepoch.set_postfix({'total_loss':t_loss , 'loss_couple': l_couple, \\\n",
        "                              'loss_e': l_e, 'loss_c': l_c})\n",
        "\n",
        "          if train_step % configs.gradient_accumulation_steps == 0:\n",
        "              optimizer.step()\n",
        "              scheduler.step()\n",
        "              model.zero_grad()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZlDqyK-jSqvj",
        "outputId": "31f4a231-2b76-4867-8b04-2a626c7bf475"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=====Epoch 1=====\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 8/1028 [00:02<05:27,  3.11batch/s, total_loss=0.153, loss_couple=0.101, loss_e=0.101, loss_c=0.103]<ipython-input-6-b26d2115713f>:271: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  rel_mask = np.array(list(map(lambda x: -k <= x <= k, rel_pos.tolist())), dtype=np.int)\n",
            "100%|██████████| 1028/1028 [06:48<00:00,  2.52batch/s, total_loss=0.000875, loss_couple=0.000392, loss_e=0.0008, loss_c=0.000558]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=====Epoch 2=====\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1028/1028 [06:43<00:00,  2.55batch/s, total_loss=0.000748, loss_couple=0.000134, loss_e=0.000631, loss_c=0.000733]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=====Epoch 3=====\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  7%|▋         | 70/1028 [00:26<05:45,  2.77batch/s, total_loss=0.0113, loss_couple=0.0029, loss_e=0.00927, loss_c=0.0104]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "for batch in tqdm(train_loader):\n",
        "  conv_len, adj_b, y_emotions_b, y_causes_b, y_mask_b, conv_couples_b, conv_id_b, \\\n",
        "  v_token_b, a_token_b, t_token_b, v_mask, a_mask, t_mask = batch\n",
        "\n",
        "  num_utt = v_token_b.size()[1]\n",
        "  outs = []\n",
        "  with torch.no_grad():\n",
        "    for i in range(num_utt):\n",
        "      v, a, t = v_token_b[:,i,:,:], a_token_b[:,i,:,:], t_token_b[:,i,:,:]\n",
        "      # print(v.size(), a.size(), t.size())\n",
        "      v_m, a_m, t_m = v_mask[:,i,:], a_mask[:,i,:], t_mask[:,i,:]\n",
        "      # print(v_m.size(), a_m.size(), t_m.size())\n",
        "      out = model(v, a, t, v_m, a_m, t_m)\n",
        "  outs = torch.stack(outs, 1)\n",
        "  break"
      ],
      "metadata": {
        "id": "4sgmMkIZ1UKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9mMStOsjaYCN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}