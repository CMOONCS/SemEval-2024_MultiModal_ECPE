{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee5e51b-0d58-4476-a6d2-2439a7a2ccf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "import cv2\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from tqdm import tqdm\n",
    "import base64\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema.messages import HumanMessage, AIMessage\n",
    "from key import OPENAI_KEY # Import your own OpenAI key.\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d9eaec-7cd8-4aad-8c42-e7af7bf4fa61",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_dir = \"../data/text/Subtask_2_test.json\" # path to json file\n",
    "anno = json.load(open(text_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10911c9-2c70-4fcd-b162-a7992ae9950b",
   "metadata": {},
   "source": [
    "#### Creating images corresponding to each video utterances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9f4687-8958-488e-a370-5aba0e012b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_text(image, text, fname):\n",
    "    \"\"\"\n",
    "    Function that writes given text on image and\n",
    "    saves the file with name as fname.\n",
    "    \"\"\"\n",
    "    width, height = 512, 192\n",
    "    im = Image.open(image)\n",
    "    draw = ImageDraw.Draw(im)\n",
    "    \n",
    "    text_width = width * 0.90\n",
    "    text_max_height = height * 0.8\n",
    "    \n",
    "    size = 36\n",
    "    # Dynamic resizing\n",
    "    while size > 1:\n",
    "        font_path = \"OpenSans-Regular.ttf\" # Insert your own font path here\n",
    "        font = ImageFont.truetype(font_path, size)\n",
    "        lines = []\n",
    "        line = \"\"\n",
    "        for word in text.split():\n",
    "          proposed_line = line\n",
    "          if line:\n",
    "            proposed_line += \" \"\n",
    "          proposed_line += word\n",
    "          if font.getlength(proposed_line) <= text_width:\n",
    "            line = proposed_line\n",
    "          else:\n",
    "            # If this word was added, the line would be too long\n",
    "            # Start a new line instead\n",
    "            lines.append(line)\n",
    "            line = word\n",
    "        if line:\n",
    "          lines.append(line)\n",
    "        text = \"\\n\".join(lines)\n",
    "        \n",
    "        x1, y1, x2, y2 = draw.multiline_textbbox((0, 0), text, font)\n",
    "        w, h = x2 - x1, y2 - y1\n",
    "        if h <= text_max_height:\n",
    "          break\n",
    "        else:\n",
    "          # The text did not fit comfortably into the image\n",
    "          # Try again at a smaller font size\n",
    "          size -= 1\n",
    "    \n",
    "    draw.multiline_text((width / 2 - w / 2 - x1, height / 2 - h / 2 - y1), text, font=font, align=\"center\", fill =(0, 0, 0))\n",
    "    im.save(\"captions/\"+fname+\".jpg\")\n",
    "\n",
    "def make_video_grid(video_path, video_dir):\n",
    "    \"\"\"\n",
    "    Reads the frames of video file at video_path and extracts 9 equidistant\n",
    "    frames from video. It then organizes the 9 frames into a 3x3 grid to make up\n",
    "    a single image. The text bar is added below the frame grid to make up the final\n",
    "    image representing the video of the utterance along with its text.\n",
    "    \"\"\"\n",
    "    # Open video\n",
    "    video_id = video_path.split(\".\")[0]\n",
    "    cap = cv2.VideoCapture(os.path.join(video_dir,video_path))\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    # Grid size\n",
    "    grid_size = 3\n",
    "    \n",
    "    # Frame skipping interval\n",
    "    if frame_count >= 9:\n",
    "    frame_skip = frame_count // 9\n",
    "    else:\n",
    "    print(\"frame count:\",frame_count)\n",
    "    frame_skip = 1\n",
    "    # Initialize frame counter\n",
    "    frame_num = 1\n",
    "    \n",
    "    # Initialize output image\n",
    "    grid_image = np.zeros((frame_height*grid_size, frame_width*grid_size, 3), np.uint8)\n",
    "    \n",
    "    frame_buffer = []\n",
    "    while cap.isOpened():\n",
    "      ret, frame = cap.read()\n",
    "    \n",
    "      if ret == True:\n",
    "          # Skip frames\n",
    "          if frame_num % frame_skip == 0: frame_buffer.append(frame)\n",
    "          frame_num += 1\n",
    "          if len(frame_buffer) == grid_size*grid_size:\n",
    "              break\n",
    "      else:\n",
    "          break\n",
    "    cap.release()\n",
    "\n",
    "    # Load text bar\n",
    "    text = cv2.imread(\"captions/\"+video_id+\".jpg\")\n",
    "    num_frames = 0\n",
    "    for i in range(grid_size):\n",
    "    for j in range(grid_size):\n",
    "      if num_frames < len(frame_buffer):\n",
    "        grid_image[i*frame_height:(i+1)*frame_height, j*frame_width:(j+1)*frame_width] = frame_buffer[i*grid_size+j]\n",
    "        num_frames += 1\n",
    "      else:\n",
    "        break\n",
    "      \n",
    "    image = cv2.resize(grid_image, (512, 320))\n",
    "    image = np.vstack([image, text])\n",
    "    cv2.imwrite(\"frames/\"+video_id+\".jpg\", image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76a3f02-a4b0-45e8-b31c-acae92a343c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a white bar which will be added below the frames\n",
    "# Containing the speaker and utterance content.\n",
    "\n",
    "bar = np.ones((512 - 320, 512, 3), np.uint8) * 255\n",
    "cv2.imwrite(\"bar.jpg\", bar)\n",
    "\n",
    "os.makedirs(\"frames\", exist_ok=True)\n",
    "os.makedirs(\"captions\", exist_ok=True)\n",
    "\n",
    "# Creating all possible white bar to be added to each utterance images.\n",
    "for an in anno:\n",
    "  conv_id = an['conversation_ID']\n",
    "  conversation = an['conversation']\n",
    "  for utt in conversation:\n",
    "    write_text(\"bar.jpg\", f\"{utt['speaker']}: \\\"{utt['text']}\\\"\", f\"dia{conv_id}utt{utt['utterance_ID']}\")\n",
    "\n",
    "# Creating images for all utterance videos\n",
    "for an in tqdm(anno):\n",
    "  conv_id = an['conversation_ID']\n",
    "  conversation = an['conversation']\n",
    "  for utt in conversation:\n",
    "    video_path = utt['video_name']\n",
    "    if not os.path.exists(\"frames/\" + video_path.split(\".\")[0] + \".jpg\"):\n",
    "      make_video_grid(video_path, \"eval_videos/videos/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c94848-aa34-464c-b26e-bade39790965",
   "metadata": {},
   "source": [
    "#### Prompting GPT-4 Vision API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05035d74-25d0-4934-b4c3-9c9bea864bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conv_imgs_batched(conversation, batch_size=10):\n",
    "    \"\"\"\n",
    "    Utterance images belonging to a conversation are batched to a particular batch_size.\n",
    "    \"\"\"\n",
    "    conv_imgs = []\n",
    "    for utt in conversation:\n",
    "        img_path = \"frames/\" + utt['video_name'].split(\".\")[0] + \".jpg\"\n",
    "        conv_imgs.append(encode_image(img_path))\n",
    "    \n",
    "    k = int(np.ceil(len(conv_imgs)/float(batch_size)))\n",
    "    batches = []\n",
    "    for i in range(k):\n",
    "        batches.append(conv_imgs[i*batch_size: (i+1)*batch_size])\n",
    "    return batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54411a8d-9f47-4bad-a392-1f63b846b40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = ChatOpenAI(model=\"gpt-4-vision-preview\",openai_api_key=OPENAI_KEY)\n",
    "\n",
    "def get_caption(batch):\n",
    "    \"\"\"\n",
    "    Prompts GPT-4 Vision API to describe a series of utterance images from a conversation where each utterance\n",
    "    image represents 9 frames in a 3x3 grid from the utterance video along with text below for what the speaker \n",
    "    was saying.\n",
    "    \"\"\"\n",
    "    out = chain.invoke(\n",
    "        [   \n",
    "            AIMessage(\n",
    "                content=\"You are an expert of Friends TV Show. You can understand a video scene from a few of its frames shown in sequence. You give precise descriptive analysis.\"\n",
    "                ),\n",
    "            HumanMessage(\n",
    "                    content=[\n",
    "                        \"Describe what is likely going on in following images of video frames of each utterance in conversation. The caption below provides speaker context. Give output as:\\n\\\n",
    "                        Scene Description: {}\",\n",
    "                        *map(lambda x: {\"image\": x}, batch),\n",
    "                    ]\n",
    "                )\n",
    "        ]\n",
    "    )\n",
    "    return out.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d53f4d-6f9a-4552-8c7d-ba82277d644a",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_raw_out = {}\n",
    "# video_raw_out = json.load(open(\"eval_raw_out.json\")) # if resuming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfa1ff7-9c4b-476a-b382-436aafca6d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing all the conversations to get video captions\n",
    "# Save steps and sleeps are to make sure that progress is not lost due to hitting\n",
    "# GPT-4 API Rate Limits. Please be careful!!\n",
    "\n",
    "save_step = 10\n",
    "for i,an in enumerate(anno):\n",
    "    conv_id = an['conversation_ID']\n",
    "    if str(conv_id) in video_raw_out or conv_id in video_raw_out: continue\n",
    "    batches = get_conv_imgs_batched(an['conversation'])\n",
    "    outputs = []\n",
    "    for batch in tqdm(batches):\n",
    "        out = get_caption(batch)\n",
    "        outputs.append(out)\n",
    "        time.sleep(0.5)\n",
    "    video_raw_out[conv_id] = outputs\n",
    "    print(\"[{}/{}] Processed Conv {}\".format(i+1, len(anno), conv_id))\n",
    "    if i%save_step == 0:\n",
    "        print(\"json dump...\")\n",
    "        json.dump(video_raw_out, open(\"eval_raw_out.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6697007e-5e58-42a9-8215-02743eca8406",
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(video_raw_out, open(\"eval_raw_out.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b44877-ffbc-46db-a5a5-5f9935e7520f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Postprocessing Raw Batched Video Captions to Coherent Caption for Whole Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f401fea-02e4-4fd5-a81d-737ea00828cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_raw_out = json.load(open(\"eval_raw_out.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc54268a-0af2-4f20-9daa-2e0d3f589f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_ids = []\n",
    "descs = []\n",
    "for conv_id, outs in video_raw_out.items():\n",
    "    desc = \"\\n\".join(outs)\n",
    "    conv_ids.append(conv_id)\n",
    "    descs.append({\"description\": desc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18da63fe-959f-4e24-b87d-b74ec7e42cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_prompt = \\\n",
    "\"\"\"\n",
    "Following is a descriptions of video clip from Friends TV show for a particular conversation.\n",
    "The descriptions are broken from each other. Stitch the description into a continous coherent\n",
    "narrative of the whole scene\n",
    "{description}\n",
    "\"\"\"\n",
    "summary_prompt = ChatPromptTemplate.from_template(summary_prompt)\n",
    "\n",
    "model = ChatOpenAI(openai_api_key=OPENAI_KEY)\n",
    "output_parser = StrOutputParser()\n",
    "description_chain = summary_prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26db485e-0a5f-4600-b219-462c2b7ef896",
   "metadata": {},
   "outputs": [],
   "source": [
    "outs = description_chain.batch(descs, config={\"max_concurrency\": 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28716bbe-49a2-4de8-94c6-ff1b01ad841c",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_proc_out = {}\n",
    "for i, out in enumerate(outs):\n",
    "    video_proc_out[conv_ids[i]] = out    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dc866c-f70c-427d-97a6-a0ea323c6909",
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(video_proc_out, open(\"eval_proc_out.json\", \"w\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
